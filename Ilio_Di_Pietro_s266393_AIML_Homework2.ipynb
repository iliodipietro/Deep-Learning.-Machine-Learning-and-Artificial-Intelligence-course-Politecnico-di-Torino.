{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QcGnGPdX2C",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9O3aM3Tb28q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "8ad60443-de33-483f-be69-efd648bad9ff"
      },
      "source": [
        "!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.4.2'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\n",
        "!pip install --upgrade pillow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.17.4)\n",
            "Requirement already satisfied: torchvision==0.4.2 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.12.0)\n",
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.3.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (6.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.17.4)\n",
            "Requirement already satisfied: Pillow-SIMD in /usr/local/lib/python3.6/dist-packages (6.0.0.post0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already up-to-date: pillow in /usr/local/lib/python3.6/dist-packages (6.2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo942LMOdlh4",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokFOdD1dJEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDLJuIXK_vh",
        "colab_type": "text"
      },
      "source": [
        "**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5PkYfqfK_SA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 101 \n",
        "\n",
        "#no transfer alexnet \n",
        "BATCH_SIZE = [320, 256]\n",
        "LR = [3e-2, 2e-2]\n",
        "\n",
        "#transfer alexnet\n",
        "BATCH_SIZE_TRANSFER = [150, 256, 512]\n",
        "LR_TRANSFER = [1e-3, 1e-4, 6e-3]\n",
        "\n",
        "MOMENTUM = 0.9      \n",
        "WEIGHT_DECAY = 5e-5  \n",
        "\n",
        "NUM_EPOCHS = 30            \n",
        "STEP_SIZE = 25\n",
        "GAMMA = 0.1 \n",
        "\n",
        "LOG_FREQUENCY = 10\n",
        "\n",
        "#resnet\n",
        "BATCH_SIZE_RESNET = [32]\n",
        "LR_RESNET = [1e-2]\n",
        "NUM_EPOCHS_RESNET = 15\n",
        "STEP_SIZE_RESNET = 12\n",
        "NUM_EPOCHS_RESNET_TRANSFER = 30\n",
        "STEP_SIZE_RESNET_TRANSFER = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwii0TBHvzh",
        "colab_type": "text"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUDdw4j2H0Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#1-2\n",
        "#AlexNet and ResNet without transfer learning\n",
        "train_transform = transforms.Compose([transforms.Resize(256), \n",
        "                                      transforms.CenterCrop(224),  \n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])\n",
        "'''\n",
        "\n",
        "\n",
        "# 3- TRANSFER LEARNING WITH ALEXNET AND RESNET\n",
        "\n",
        "normalize = transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "train_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                       transforms.CenterCrop(224),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       normalize,])\n",
        "\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      normalize,])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6Ev95-Wsbws",
        "colab_type": "text"
      },
      "source": [
        "Caltech_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K81qIdqksW3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.datasets import VisionDataset\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import sys\n",
        "\n",
        "\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "\n",
        "def make_dataset(dir, label_idx):\n",
        "  images = []\n",
        "  for target in label_idx.keys():\n",
        "    d = os.path.join(dir, target)\n",
        "    files = os.listdir(d)\n",
        "    for file in files:\n",
        "      item = (os.path.join(target, file), label_idx[target])  \n",
        "      images.append(item)\n",
        "  return images\n",
        "\n",
        "class Caltech(VisionDataset):\n",
        "    def __init__(self, root, split='train', transform = None, target_transform = None):\n",
        "        super(Caltech, self).__init__(root, transform = transform, target_transform = target_transform)\n",
        "\n",
        "        self.split = split \n",
        "        self.root = root\n",
        "        self.transform = transform \n",
        "        self.target_transform = target_transform \n",
        "        \n",
        "        self.label_list = [d for d in os.listdir(self.root) if d != 'BACKGROUND_Google'] #creo lista delle lables escludendo background\n",
        "        self.label_list = sorted(self.label_list, key = str.casefold)\n",
        "        idx = 0\n",
        "        self.label_idx = {self.label_list[idx]: idx for idx in range(len(self.label_list))}#lista di indici delle lables\n",
        "        self.samples = make_dataset(self.root, self.label_idx)\n",
        "\n",
        "        #reading the splits files\n",
        "        file = os.path.join('Homework2-Caltech101', split)\n",
        "        i = 0\n",
        "        self.dataset = []\n",
        "\n",
        "        for img, labels in self.samples:\n",
        "          with open(file) as f:\n",
        "            if(img in f.read()):\n",
        "              self.dataset.append(i)\n",
        "          i += 1    \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        idx = self.dataset[index]\n",
        "        image, label = self.samples[idx]\n",
        "        image = os.path.join('Homework2-Caltech101/101_ObjectCategories', image)\n",
        "        sample = pil_loader(image)\n",
        "        # Applies preprocessing when accessing the image\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(sample)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        length = len(self.dataset) \n",
        "        return length\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qYIHPzYLY7i",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfVq_uDHLbsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone github repository with data\n",
        "if not os.path.isdir('./Homework2-Caltech101'):\n",
        "  !git clone https://github.com/MachineLearning2020/Homework2-Caltech101.git\n",
        "\n",
        "DATA_DIR = 'Homework2-Caltech101/101_ObjectCategories'\n",
        "\n",
        "#PART 1-2 -3\n",
        "train_dataset_t = Caltech(DATA_DIR, 'train.txt', transform = train_transform)\n",
        "train_dataset_v = Caltech(DATA_DIR, 'train.txt', transform = train_transform)\n",
        "test_dataset = Caltech(DATA_DIR,    'test.txt', transform = eval_transform)\n",
        "\n",
        "val_indexes = [idx for idx in range(len(train_dataset_v)) if idx % 2]\n",
        "train_indexes = [idx for idx in range(len(train_dataset_t)) if not idx % 2]\n",
        "\n",
        "val_dataset = Subset(train_dataset_v, val_indexes)\n",
        "train_dataset = Subset(train_dataset_t, train_indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdSls25NlN5m",
        "colab_type": "text"
      },
      "source": [
        "**Computation of accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDsNkk3NL-VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_accuracy(net, dataloader, type_of_set):  \n",
        "    ########TESTING PHASE###########\n",
        "\n",
        "    # check accuracy on whole test set\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    net.train(False)  \n",
        "    with torch.no_grad(): \n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            outputs = net(images)  # predictions\n",
        "            _, predicted = torch.max(outputs.data, 1)  # predicted labels\n",
        "            total += labels.size(0)\n",
        "            correct += torch.sum(predicted == labels.data).data.item()  # compare with ground truth\n",
        "    accuracy = 100 * correct / total\n",
        "    print('Accuracy of the network on the %s set: %d %%' %(type_of_set, accuracy))\n",
        "    net.train(True)\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYl_rk6Chz0o",
        "colab_type": "text"
      },
      "source": [
        "**Evaluation on Test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5ajFLsxh0ew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(net):\n",
        "  best_net = net.to(DEVICE) \n",
        "  best_net.train(False) \n",
        "\n",
        "  running_corrects = 0\n",
        "  for images, labels in tqdm(test_dataloader):\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    # Forward Pass\n",
        "    outputs = best_net(images)\n",
        "\n",
        "    # Get predictions\n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Update Corrects\n",
        "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  accuracy = 100*running_corrects / float(len(test_dataset))\n",
        "\n",
        "  print('Test Accuracy: {}'.format(accuracy))\n",
        "  best_net.train(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqO-ItQzlVxD",
        "colab_type": "text"
      },
      "source": [
        "**Train an Validation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XUz4_TU86Zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_and_val(net, n_epoch, optimizer,scheduler, train_dataloader, val_dataloader):\n",
        "\n",
        "  losses = np.empty(n_epoch)\n",
        "  j = 0\n",
        "  current_step = 0\n",
        "  accuracies_train = np.empty(n_epoch)\n",
        "  accuracies_val =  np.empty(n_epoch)\n",
        "  n_loss_print = len(train_dataloader)\n",
        "  criterion = nn.CrossEntropyLoss() \n",
        "\n",
        "   # Start iterating over the epochs\n",
        "  for epoch in range(n_epoch):\n",
        "    running_loss = 0.0\n",
        "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, n_epoch, scheduler.get_lr()))\n",
        "    # Iterate over the dataset\n",
        "    for images, labels in train_dataloader:\n",
        "      # Bring data over the device of choice\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      net.train() \n",
        "\n",
        "      optimizer.zero_grad() \n",
        "\n",
        "      outputs = net(images)\n",
        "\n",
        "      loss = criterion(outputs, labels)\n",
        "      running_loss += loss.item()\n",
        "      \n",
        "      if current_step % LOG_FREQUENCY == 0:\n",
        "        print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "      loss.backward()  \n",
        "      optimizer.step() \n",
        "\n",
        "      current_step += 1\n",
        "    \n",
        "    \n",
        "    losses[j] = running_loss / n_loss_print\n",
        "    accuracies_train[j] = test_accuracy(net, train_dataloader, 'train')  # at each epoch\n",
        "\n",
        "    #EVALUATION ON VALIDATION\n",
        "    accuracies_val[j] = test_accuracy(net, val_dataloader, 'validation')  # at each epoch\n",
        "    j += 1\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step() \n",
        "\n",
        "\n",
        "  #print dei plot\n",
        "  plt.plot(accuracies_train, 'r', label = 'Accuracy on Train')\n",
        "  plt.plot(accuracies_val,   'g', label = 'Accuracy on Validation')\n",
        "  plt.legend(loc = 'best')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy values')\n",
        "  plt.grid()\n",
        "  plt.figure()\n",
        "  plt.show()\n",
        "  plt.plot(losses,           'b', label = 'Training Loss') \n",
        "  plt.legend(loc = 'best')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss values')\n",
        "  plt.grid()\n",
        "  plt.figure()\n",
        "  plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxYUli9d9uYQ",
        "colab_type": "text"
      },
      "source": [
        "**Train without transfer learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcoQ5fD49yT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PART 1-2\n",
        "i = 0\n",
        "for i in range(len(LR_RESNET)): # use LR_RESNET for ResNet, LR for AlexNet\n",
        "  #AlexNet\n",
        "  '''\n",
        "  net = alexnet()#(comment this part if you want to use resnet)\n",
        "  net.classifier[6] = nn.Linear(4096, NUM_CLASSES)#(comment this part if you want to use resnet)\n",
        "  '''\n",
        "  \n",
        "  #ResNet\n",
        "  net = torch.hub.load('pytorch/vision:v0.4.2', 'resnet101')#(comment this part if you want to use alexnet)\n",
        "  net.fc = nn.Linear(2048, NUM_CLASSES)#(comment this part if you want to use alexnet)\n",
        "  \n",
        "  net = net.to(DEVICE)\n",
        "  \n",
        "  n_epoch = NUM_EPOCHS              #use NUM_EPOCHS_RESNET for resnet\n",
        "  learning_rate = LR_RESNET[i]      #use LR_RESNET for ResNet, LR for AlexNet\n",
        "  ss = STEP_SIZE                    #use STEP_SIZE_RESNET for resnet\n",
        "  bs = BATCH_SIZE_RESNET[i]         #use BATCH_SIZE_RESNET for ResNet, BATCH_SIZE for AlexNet\n",
        "\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size = bs, shuffle = True,  num_workers = 4, drop_last = True)\n",
        "  val_dataloader   = DataLoader(val_dataset,   batch_size = bs, shuffle = True,  num_workers = 4)\n",
        "  \n",
        "  cudnn.benchmark\n",
        "  parameters_to_optimize = net.parameters()\n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr = learning_rate, momentum = MOMENTUM, weight_decay = WEIGHT_DECAY)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = ss, gamma = GAMMA)\n",
        "\n",
        "  training_and_val(net, n_epoch, optimizer,scheduler, train_dataloader, val_dataloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08CVoqoQRX2m",
        "colab_type": "text"
      },
      "source": [
        "**Transfer Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60XUCnxNRZNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PART 3 TRANSFER LEARNING\n",
        "i = 0\n",
        "\n",
        "for i in range(len(LR_RESNET)):# use LR_RESNET for ResNet, LR_TRANSFER for AlexNet\n",
        "\n",
        "  #AlexNet\n",
        "  '''\n",
        "  net = alexnet(pretrained = True)#(comment this part if you want to use resnet)\n",
        "  net.classifier[6] = nn.Linear(4096, NUM_CLASSES)#(comment this part if you want to use resnet)\n",
        "  '''\n",
        "  #ResNet\n",
        "  \n",
        "  net = torch.hub.load('pytorch/vision:v0.4.2', 'resnet101', pretrained = True)#(comment this part if you want to use alexnet)\n",
        "  net.fc = nn.Linear(2048, NUM_CLASSES)#(comment this part if you want to use alexnet)\n",
        "\n",
        "  net = net.to(DEVICE)\n",
        "  \n",
        "  n_epoch =  NUM_EPOCHS           #use NUM_EPOCHS_RESNET_TRANSFER for resnet\n",
        "  learning_rate = LR_RESNET[i]    #use LR_RESNET for ResNet, LR_TRANSFER for AlexNet\n",
        "  ss = STEP_SIZE                  #use STEP_SIZE_RESNET_TRANSFER for resnet\n",
        "  bs = BATCH_SIZE_RESNET[i]       #use BATCH_SIZE_RESNET for resnet, BATCH_SIZE_TRANSFER for AlexNet\n",
        "\n",
        "  #Prepare Training\n",
        "  #for tuning of BATCH_SIZE\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size = bs, shuffle = True,  num_workers = 4, drop_last = True)\n",
        "  val_dataloader   = DataLoader(val_dataset,   batch_size = bs, shuffle = True,  num_workers = 4)\n",
        "  \n",
        "  cudnn.benchmark # Calling this optimizes runtime\n",
        "  parameters_to_optimize = net.parameters()\n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr = learning_rate, momentum = MOMENTUM, weight_decay = WEIGHT_DECAY)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = ss, gamma = GAMMA)\n",
        "\n",
        "  training_and_val(net, n_epoch, optimizer, scheduler, train_dataloader, val_dataloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTUdKramSFK8",
        "colab_type": "text"
      },
      "source": [
        "**Freeze**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv-XSN0rSJQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#qui definisco a mano i migliori iperparametri dati dal run con transfer learning\n",
        "best_lr = 1e-3\n",
        "best_bs = 150\n",
        "best_epoch = 30\n",
        "best_step_size = 25\n",
        "\n",
        "#tuning of batch_size\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = best_bs, shuffle = True,  num_workers = 4, drop_last = True)\n",
        "val_dataloader   = DataLoader(val_dataset,   batch_size = best_bs, shuffle = True,  num_workers = 4)\n",
        "test_dataloader  = DataLoader(test_dataset,  batch_size = best_bs, shuffle = False, num_workers = 4)\n",
        "\n",
        "n_loss_print = len(train_dataloader)\n",
        "v = [0, 1]\n",
        "i = 0\n",
        "for i in v:\n",
        "\n",
        "  new_net = alexnet(pretrained = True)\n",
        "  new_net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "\n",
        "  new_net = new_net.to(DEVICE)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  cudnn.benchmark \n",
        "  if i == 0:\n",
        "    # train of only the fully connected layers(freeze of convolutional)\n",
        "    parameters_to_optimize = new_net.classifier.parameters()\n",
        "  else:\n",
        "    # train of only the conv layers(freeze of fully connectes)\n",
        "    parameters_to_optimize = new_net.features.parameters()\n",
        "\n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr = best_lr, momentum = MOMENTUM, weight_decay = WEIGHT_DECAY)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = best_step_size, gamma = GAMMA)\n",
        "  training_and_val(new_net, best_epoch, optimizer, scheduler, train_dataloader, val_dataloader)\n",
        "  evaluate(new_net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaWYsney2vJR",
        "colab_type": "text"
      },
      "source": [
        "**Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvHF7t1t1o4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#part 4 - Data Augmentation\n",
        "#qui definisco a mano i migliori iperparametri dati dal run con transfer learning\n",
        "best_lr = 1e-3\n",
        "best_bs = 150\n",
        "best_epoch = 30\n",
        "best_step_size = 25\n",
        "\n",
        "#ridefinisco i dataloader perche ho trovato un migliore batch size\n",
        "val_dataloader   = DataLoader(val_dataset,   batch_size = best_bs, shuffle=True,  num_workers=4)\n",
        "test_dataloader  = DataLoader(test_dataset,  batch_size = best_bs, shuffle=False, num_workers=4)\n",
        "\n",
        "for i in [0, 1, 2]:\n",
        "  normalize = transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "  #define new transformation and train set\n",
        "  if(i == 0):\n",
        "    new_transform = transforms.Compose([transforms.RandomResizedCrop(224),   \n",
        "                                          transforms.RandomGrayscale(), \n",
        "                                          transforms.ToTensor(), \n",
        "                                          normalize]) \n",
        "    \n",
        "  else:\n",
        "    if(i == 1):\n",
        "      new_transform = transforms.Compose([transforms.RandomResizedCrop(224),   \n",
        "                                          transforms.RandomHorizontalFlip(), \n",
        "                                          transforms.ToTensor(), \n",
        "                                          normalize])  \n",
        "    else:\n",
        "      new_transform = transforms.Compose([transforms.RandomResizedCrop(224),   \n",
        "                                          transforms.ToTensor(), \n",
        "                                          normalize,\n",
        "                                          transforms.RandomErasing()]) \n",
        "      \n",
        "  train_dataset_t_da = Caltech(DATA_DIR, 'train.txt', transform = new_transform)\n",
        "\n",
        "  train_indexes_da = [idx for idx in range(len(train_dataset_t_da)) if not idx % 2]\n",
        "\n",
        "  train_dataset_da = Subset(train_dataset_t_da, train_indexes_da)\n",
        "  #ridefinisco il dataloader perche ho trovato il migliore batch size ed ho una nuova trasformazione\n",
        "  train_dataloader_da = DataLoader(train_dataset_da, batch_size = best_bs, shuffle = True,  num_workers = 4, drop_last = True)\n",
        "\n",
        "  #alexnet\n",
        "\n",
        "  net = alexnet(pretrained = True)\n",
        "  net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "\n",
        "  net = net.to(DEVICE)\n",
        "\n",
        "  cudnn.benchmark\n",
        "  parameters_to_optimize = net.parameters()\n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr = best_lr, momentum = MOMENTUM, weight_decay = WEIGHT_DECAY)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = best_step_size, gamma = GAMMA)\n",
        "\n",
        "  training_and_val(net, best_epoch, optimizer, scheduler, train_dataloader_da, val_dataloader)\n",
        "  evaluate(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASIUDq0AbN6O",
        "colab_type": "text"
      },
      "source": [
        "**Evaluation on test set with best Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbDMCWrFbMs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#valori da aggiungere a run precedente finita (ogni volta che ricerco i migliori iperparametri eseguo questo blocco per riallenare la rete e ottenere pesi coerenti)\n",
        "best_lr = 1e-3\n",
        "best_bs = 150\n",
        "best_epoch = 30\n",
        "best_step_size = 25\n",
        "\n",
        "#ridefinisco la rete\n",
        "#alexnet(comment this part if you want to use resnet)\n",
        "\n",
        "best_net = alexnet(pretrained = True) #best_net = alexnet() without transfer learning (comment this part if you want to use resnet)\n",
        "best_net.classifier[6] = nn.Linear(4096, NUM_CLASSES)#(comment this part if you want to use resnet)\n",
        "\n",
        "#resnet\n",
        "'''\n",
        "best_net = torch.hub.load('pytorch/vision:v0.4.2', 'resnet101')#best_net = torch.hub.load('pytorch/vision:v0.4.2', 'resnet101', pretrained=True) for transfer learning(comment this part if you want to use alexnet)\n",
        "best_net.fc = nn.Linear(2048, NUM_CLASSES)#(comment this part if you want to use alexnet)\n",
        "'''\n",
        "best_net = best_net.to(DEVICE)# this will bring the network to GPU if DEVICE is cuda\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "parameters_to_optimize = best_net.parameters()\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr = best_lr, momentum = MOMENTUM, weight_decay = WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = best_step_size, gamma = GAMMA)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = best_bs, shuffle=True,  num_workers=4, drop_last=True)\n",
        "val_dataloader   = DataLoader(val_dataset,   batch_size = best_bs, shuffle=True,  num_workers=4)\n",
        "test_dataloader  = DataLoader(test_dataset,  batch_size = best_bs, shuffle=False, num_workers=4)\n",
        "#rialleno la rete per avere pesi aggiornati con gli iperparametri migliori\n",
        "training_and_val(best_net, best_epoch, optimizer, scheduler, train_dataloader, val_dataloader)\n",
        "evaluate(best_net)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}